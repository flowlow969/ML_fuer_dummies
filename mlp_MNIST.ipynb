{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Intro\n",
    "\n",
    "This NB is constructed such that it also runs in reasonable time on Laptop CPUs (e.g. an i3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "There are different options to set up the TensorFlow library (which now includes [Keras](https://keras.io) as backend library) on your own computer. The simplest of them is using only the CPU and can be installed in 1 command via [`conda`](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/), in an anaconda shell run:\n",
    "\n",
    "```\n",
    "conda install tensorflow\n",
    "```\n",
    "\n",
    "**NOTE**: TF migth not be compatible with your current environment, so here we create a [new environment](https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands) first:\n",
    "\n",
    "```\n",
    "conda create -n tf tensorflow\n",
    "conda activate tf\n",
    "```\n",
    "\n",
    "In that case you need to install jupyter, scikit-learn, matplotlib, numpy and pandas in that environment again, with e.g.:\n",
    "\n",
    "```\n",
    "conda install jupyter scikit-learn matplotlib numpy pandas\n",
    "```\n",
    "\n",
    "(If you have a [supported Nvidia graphics card](https://developer.nvidia.com/cuda-gpus) in your machine and would like to use it for accelerated network training, make sure to follow [this guide](https://www.tensorflow.org/install/gpu) to install required packages and finally use the `tensorflow-gpu` library.)\n",
    "\n",
    "The usage of the TensorFlow library in Python will be the same for CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Scale the input data into the range [0, 1]\n",
    "X= mnist.data\n",
    "X= X / 255.0\n",
    "y= mnist.target\n",
    "## use sklearn's train_test_split to split the data into \n",
    "## 50000 instances for training (X_train, y_train), 10000 for validation (X_val, y_val) and 10000 for testing (X_test, y_test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_val_size = 2/7\n",
    "test_size = 1/2\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_val_size, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=test_size, random_state=42)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 2, Number of output neurons: 10\n"
     ]
    }
   ],
   "source": [
    "## load an MLP classifier from sklearn with all its defaults, only specifying `random_state=42`\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=42).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## try printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n",
    "print(\"Layers: \" + str(len(clf.coefs_)) + \", Number of output neurons: \" + str(len(clf.classes_)))\n",
    "## train the MLP with the train set, time its execution\n",
    "\n",
    "## try again printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000000\n",
      "Test set score: 0.976100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## print the scores of the trained MLP on the train and on the test set:\n",
    "print(\"Training set score: %f\" % clf.score(X_train, y_train) )\n",
    "print(\"Test set score: %f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 1\n",
    "\n",
    "1. What are the default values assumed for the MLPClassifier of sklearn?\n",
    "2. What MLP is constructed with the defaults? \\\n",
    "   I.e. how many hidden layers and how many input, hidden and output neurons/units does the MLP have?\n",
    "\n",
    "### Answers\n",
    "\n",
    "1. \n",
    "    - hidden_layer_sizestuple, length = n_layers - 2, default=(100,)\n",
    "    - len(clf.coefs_)\n",
    "    - solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n",
    "    - alphafloat, default=0.0001\n",
    "    - batch_sizeint, default=’auto’\n",
    "    - learning_rate{‘constant’, ‘invscaling’, ‘adaptive’}, default=’constant’\n",
    "    - learning_rate_initdouble, default=0.001\n",
    "    - power_tdouble, default=0.5\n",
    "    - max_iterint, default=200\n",
    "    - shufflebool, default=True\n",
    "    - random_stateint, RandomState instance, default=None\n",
    "    - tolfloat, default=1e-4verbosebool, default=False\n",
    "    - momentumfloat, default=0.9\n",
    "    - early_stoppingbool, default=False\n",
    "    - validation_fractionfloat, default=0.1\n",
    "    - beta_1float, default=0.9\n",
    "    - beta_2float, default=0.999\n",
    "    - epsilonfloat, default=1e-8\n",
    "    - n_iter_no_changeint, default=10\n",
    "    - max_funint, default=15000\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers: 4, Number of Hidenlayers: 2, Number of output neurons: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.59856198e-14, 1.77106890e-15, 2.83380745e-13, 1.77307736e-16,\n",
       "        2.59506297e-16, 3.00159860e-10, 5.28521229e-19, 8.29700347e-14,\n",
       "        1.00000000e+00, 1.09165300e-16]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now construct another MLP classifier as above but with 2 hidden layers of 100 and 50 neurons/units.\n",
    "## In addition it should used mini-batch gradient descent (mBGD) with a mini-batch size of 100\n",
    "## and train only for 100 epochs.\n",
    "## Read the docs carefully to figure out what you need to specify!\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50),random_state=42, batch_size=min(100,50000),max_iter=100)\n",
    "\n",
    "## try printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n",
    "## train the MLP with the train set, time its execution\n",
    "clf = clf.fit(X_train, y_train)\n",
    "## try again printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n",
    "print(\"Number of Layers: \" + str(clf.n_layers_) + \", Number of Hidenlayers: \" + str(clf.n_layers_-2) + \", Number of output neurons: \" + str(clf.n_outputs_))\n",
    "clf.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000000\n",
      "Test set score: 0.976500\n"
     ]
    }
   ],
   "source": [
    "## print the scores of the trained MLP on the train and on the test set:\n",
    "print(\"Training set score: %f\" % clf.score(X_train, y_train) )\n",
    "print(\"Test set score: %f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84060"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def n_params(model): # from: https://stackoverflow.com/questions/59078110/way-to-count-the-number-of-parameters-in-a-scikit-learn-model\n",
    "    \"\"\"Return total number of parameters in a \n",
    "    Scikit-Learn model.\n",
    "\n",
    "    This works for the following model types:\n",
    "     - sklearn.neural_network.MLPClassifier\n",
    "     - sklearn.neural_network.MLPRegressor\n",
    "     - sklearn.linear_model.LinearRegression\n",
    "     - and maybe some others\n",
    "    \"\"\"\n",
    "    return (sum([a.size for a in model.coefs_]) +  \n",
    "            sum([a.size for a in model.intercepts_]))\n",
    "\n",
    "## use the given function to get the number of model parameters of the last MLP\n",
    "n_params(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 2\n",
    "\n",
    "1. Does the returned number of parameters match your expectations? Write down your own calculation!\n",
    "\n",
    "\n",
    "### Answers\n",
    "\n",
    "1. \n",
    "   - Inputlayer  784 neuronen\n",
    "   - Hidenlayer1 100 neuronen\n",
    "   - Hidenlayer2  50 neuronen\n",
    "   - Outputlayer  10 neuronen\n",
    "   \n",
    "   n_params = Inputlayer * Hidenlayer1 + Hidenlayer1 * Hidenlayer2 + Hidenlayer2 * Outputlayer + Hidenlayer1 + Hidenlayer2 + Outputlayer\n",
    "   \n",
    "   n_params = 784 * 100 + 100 * 50 + 50 * 10 + 100 + 50 + 10 = 84060\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now use the example from: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html\n",
    "## to plot ALL weight matrices of the first layer of the MLP trained above\n",
    "## using subplots with 20 columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test your TensorFlow installation by importing the package. The following code cell should execute without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check which computing devices TensorFlow has found on this machine. If you don't have the GPU setup on your computer, the list should just contain one CPU: `/device:CPU:0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a similar MLP as above using tf.keras, see also this [tutorial network](https://github.com/keras-team/keras/blob/fcc0bfa354c5a47625d681d0297a66ef9ff43a9e/examples/mnist_mlp.py) which also uses the MNIST dataset.\n",
    "\n",
    "Keras has a nice method `model.summary()` that prints a tabular overview of your network architecture, together with the input/output dimensions and number of parameters for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "## convert class vectors to binary class matrices\n",
    "y_train_c = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_c = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test_c = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(180, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense( 40, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_c,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test_c, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the training going through the epochs and in the end the trained network is evaluated on the test set. \n",
    "It shoud reach at least a classification accurary of 97% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now try to tune the hyper-parameters of the MLP to achieve more than 98% accuracy on the test set.\\\n",
    "List the parameters you changed to achieve this score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
