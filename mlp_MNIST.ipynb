{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Intro\n",
    "\n",
    "This NB is constructed such that it also runs in reasonable time on Laptop CPUs (e.g. an i3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "There are different options to set up the TensorFlow library (which now includes [Keras](https://keras.io) as backend library) on your own computer. The simplest of them is using only the CPU and can be installed in 1 command via [`conda`](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/), in an anaconda shell run:\n",
    "\n",
    "```\n",
    "conda install tensorflow\n",
    "```\n",
    "\n",
    "**NOTE**: TF migth not be compatible with your current environment, so here we create a [new environment](https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands) first:\n",
    "\n",
    "```\n",
    "conda create -n tf tensorflow\n",
    "conda activate tf\n",
    "```\n",
    "\n",
    "In that case you need to install jupyter, scikit-learn, matplotlib, numpy and pandas in that environment again, with e.g.:\n",
    "\n",
    "```\n",
    "conda install jupyter scikit-learn matplotlib numpy pandas\n",
    "```\n",
    "\n",
    "(If you have a [supported Nvidia graphics card](https://developer.nvidia.com/cuda-gpus) in your machine and would like to use it for accelerated network training, make sure to follow [this guide](https://www.tensorflow.org/install/gpu) to install required packages and finally use the `tensorflow-gpu` library.)\n",
    "\n",
    "The usage of the TensorFlow library in Python will be the same for CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel1      0.000000\n",
      "pixel2      0.000000\n",
      "pixel3      0.000000\n",
      "pixel4      0.000000\n",
      "pixel5      0.000000\n",
      "              ...   \n",
      "pixel780    0.243137\n",
      "pixel781    0.000000\n",
      "pixel782    0.000000\n",
      "pixel783    0.000000\n",
      "pixel784    0.000000\n",
      "Length: 784, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24301</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52585</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13415</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62085</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36753</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39592</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62475</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46704</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8777</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "24301     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "52585     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "13415     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "28171     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "62085     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "36753     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "39592     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "62475     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "46704     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "8777      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "24301      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "52585      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "13415      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "28171      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "62085      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "36753      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "39592      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "62475      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "46704      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "8777       0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "24301       0.0       0.0       0.0       0.0       0.0  \n",
       "52585       0.0       0.0       0.0       0.0       0.0  \n",
       "13415       0.0       0.0       0.0       0.0       0.0  \n",
       "28171       0.0       0.0       0.0       0.0       0.0  \n",
       "62085       0.0       0.0       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "36753       0.0       0.0       0.0       0.0       0.0  \n",
       "39592       0.0       0.0       0.0       0.0       0.0  \n",
       "62475       0.0       0.0       0.0       0.0       0.0  \n",
       "46704       0.0       0.0       0.0       0.0       0.0  \n",
       "8777        0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[10000 rows x 784 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Scale the input data into the range [0, 1]\n",
    "X= mnist.data\n",
    "X= X / 255.0\n",
    "print(X.max())\n",
    "y= mnist.target\n",
    "## use sklearn's train_test_split to split the data into \n",
    "## 50000 instances for training (X_train, y_train), 10000 for validation (X_val, y_val) and 10000 for testing (X_test, y_test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_val_size = 2/7\n",
    "test_size = 1/2\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_val_size, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=test_size, random_state=42)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 2, Number of output neurons: 10\n"
     ]
    }
   ],
   "source": [
    "## load an MLP classifier from sklearn with all its defaults, only specifying `random_state=42`\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=42).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## try printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n",
    "print(\"Layers: \" + str(len(clf.coefs_)) + \", Number of output neurons: \" + str(len(clf.classes_)))\n",
    "## train the MLP with the train set, time its execution\n",
    "\n",
    "## try again printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000000\n",
      "Test set score: 0.976100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## print the scores of the trained MLP on the train and on the test set:\n",
    "print(\"Training set score: %f\" % clf.score(X_train, y_train) )\n",
    "print(\"Test set score: %f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 1\n",
    "\n",
    "1. What are the default values assumed for the MLPClassifier of sklearn?\n",
    "2. What MLP is constructed with the defaults? \\\n",
    "   I.e. how many hidden layers and how many input, hidden and output neurons/units does the MLP have?\n",
    "\n",
    "### Answers\n",
    "\n",
    "1. \n",
    "    hidden_layer_sizestuple, length = n_layers - 2, default=(100,)\n",
    "    len(clf.coefs_)\n",
    "    solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’\n",
    "    alphafloat, default=0.0001\n",
    "    batch_sizeint, default=’auto’\n",
    "    learning_rate{‘constant’, ‘invscaling’, ‘adaptive’}, default=’constant’\n",
    "    learning_rate_initdouble, default=0.001\n",
    "    power_tdouble, default=0.5\n",
    "    max_iterint, default=200\n",
    "    shufflebool, default=True\n",
    "    random_stateint, RandomState instance, default=None\n",
    "    tolfloat, default=1e-4verbosebool, default=False\n",
    "    momentumfloat, default=0.9\n",
    "    early_stoppingbool, default=False\n",
    "    validation_fractionfloat, default=0.1\n",
    "    beta_1float, default=0.9\n",
    "    beta_2float, default=0.999\n",
    "    epsilonfloat, default=1e-8\n",
    "    n_iter_no_changeint, default=10\n",
    "    max_funint, default=15000\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now construct another MLP classifier as above but with 2 hidden layers of 100 and 50 neurons/units.\n",
    "## In addition it should used mini-batch gradient descent (mBGD) with a mini-batch size of 100\n",
    "## and train only for 100 epochs.\n",
    "## Read the docs carefully to figure out what you need to specify!\n",
    "\n",
    "## try printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n",
    "\n",
    "## train the MLP with the train set, time its execution\n",
    "\n",
    "## try again printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the scores of the trained MLP on the train and on the test set:\n",
    "print(\"Training set score: %f\" % )\n",
    "print(\"Test set score: %f\" % )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(model): # from: https://stackoverflow.com/questions/59078110/way-to-count-the-number-of-parameters-in-a-scikit-learn-model\n",
    "    \"\"\"Return total number of parameters in a \n",
    "    Scikit-Learn model.\n",
    "\n",
    "    This works for the following model types:\n",
    "     - sklearn.neural_network.MLPClassifier\n",
    "     - sklearn.neural_network.MLPRegressor\n",
    "     - sklearn.linear_model.LinearRegression\n",
    "     - and maybe some others\n",
    "    \"\"\"\n",
    "    return (sum([a.size for a in model.coefs_]) +  \n",
    "            sum([a.size for a in model.intercepts_]))\n",
    "\n",
    "## use the given function to get the number of model parameters of the last MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 2\n",
    "\n",
    "1. Does the returned number of parameters match your expectations? Write down your own calculation!\n",
    "\n",
    "\n",
    "### Answers\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now use the example from: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html\n",
    "## to plot ALL weight matrices of the first layer of the MLP trained above\n",
    "## using subplots with 20 columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test your TensorFlow installation by importing the package. The following code cell should execute without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check which computing devices TensorFlow has found on this machine. If you don't have the GPU setup on your computer, the list should just contain one CPU: `/device:CPU:0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4973998682535495454\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 11:09:59.127835: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a similar MLP as above using tf.keras, see also this [tutorial network](https://github.com/keras-team/keras/blob/fcc0bfa354c5a47625d681d0297a66ef9ff43a9e/examples/mnist_mlp.py) which also uses the MNIST dataset.\n",
    "\n",
    "Keras has a nice method `model.summary()` that prints a tabular overview of your network architecture, together with the input/output dimensions and number of parameters for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "2.7.0\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 180)               141300    \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 40)                7240      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                410       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148,950\n",
      "Trainable params: 148,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3278 - accuracy: 0.9059 - val_loss: 0.1778 - val_accuracy: 0.9472\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1300 - accuracy: 0.9615 - val_loss: 0.1214 - val_accuracy: 0.9638\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0891 - accuracy: 0.9730 - val_loss: 0.1053 - val_accuracy: 0.9684\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0658 - accuracy: 0.9800 - val_loss: 0.0935 - val_accuracy: 0.9720\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0483 - accuracy: 0.9852 - val_loss: 0.1029 - val_accuracy: 0.9705\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0386 - accuracy: 0.9876 - val_loss: 0.0990 - val_accuracy: 0.9727\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0301 - accuracy: 0.9908 - val_loss: 0.1018 - val_accuracy: 0.9710\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0251 - accuracy: 0.9930 - val_loss: 0.0972 - val_accuracy: 0.9739\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0889 - val_accuracy: 0.9762\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.0860 - val_accuracy: 0.9769\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.0947 - val_accuracy: 0.9760\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.1075 - val_accuracy: 0.9736\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.0962 - val_accuracy: 0.9780\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.1034 - val_accuracy: 0.9749\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0098 - accuracy: 0.9968 - val_loss: 0.0936 - val_accuracy: 0.9796\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.1061 - val_accuracy: 0.9761\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9966 - val_loss: 0.1067 - val_accuracy: 0.9763\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.1193 - val_accuracy: 0.9726\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.1181 - val_accuracy: 0.9750\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9978 - val_loss: 0.1355 - val_accuracy: 0.9743\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.1475 - val_accuracy: 0.9728\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.1184 - val_accuracy: 0.9769\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.1162 - val_accuracy: 0.9786\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.1273 - val_accuracy: 0.9757\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.1358 - val_accuracy: 0.9758\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.1409 - val_accuracy: 0.9753\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.1205 - val_accuracy: 0.9781\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 9.7176e-04 - accuracy: 0.9999 - val_loss: 0.1236 - val_accuracy: 0.9781\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.4064e-04 - accuracy: 1.0000 - val_loss: 0.1188 - val_accuracy: 0.9801\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.0129e-04 - accuracy: 1.0000 - val_loss: 0.1207 - val_accuracy: 0.9798\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.1855e-05 - accuracy: 1.0000 - val_loss: 0.1218 - val_accuracy: 0.9797\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 5.7394e-05 - accuracy: 1.0000 - val_loss: 0.1228 - val_accuracy: 0.9800\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 4.8659e-05 - accuracy: 1.0000 - val_loss: 0.1241 - val_accuracy: 0.9796\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 4.0613e-05 - accuracy: 1.0000 - val_loss: 0.1252 - val_accuracy: 0.9800\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 3.5435e-05 - accuracy: 1.0000 - val_loss: 0.1279 - val_accuracy: 0.9805\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 2.9550e-05 - accuracy: 1.0000 - val_loss: 0.1279 - val_accuracy: 0.9803\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.6052e-05 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9805\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9952 - val_loss: 0.1933 - val_accuracy: 0.9638\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 0.1458 - val_accuracy: 0.9748\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9981 - val_loss: 0.1357 - val_accuracy: 0.9771\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.1333 - val_accuracy: 0.9778\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.1369 - val_accuracy: 0.9777\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.1733 - val_accuracy: 0.9739\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.1364 - val_accuracy: 0.9782\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.1492 - val_accuracy: 0.9763\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.1466 - val_accuracy: 0.9782\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.1401 - val_accuracy: 0.9795\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.1739 - val_accuracy: 0.9769\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.1500 - val_accuracy: 0.9763\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.1341 - val_accuracy: 0.9797\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 2.0935e-04 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9804\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 6.0477e-05 - accuracy: 1.0000 - val_loss: 0.1305 - val_accuracy: 0.9808\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 3.9483e-05 - accuracy: 1.0000 - val_loss: 0.1316 - val_accuracy: 0.9807\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 3.2024e-05 - accuracy: 1.0000 - val_loss: 0.1329 - val_accuracy: 0.9809\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 2.6682e-05 - accuracy: 1.0000 - val_loss: 0.1342 - val_accuracy: 0.9810\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.2201e-05 - accuracy: 1.0000 - val_loss: 0.1358 - val_accuracy: 0.9807\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 1.8698e-05 - accuracy: 1.0000 - val_loss: 0.1370 - val_accuracy: 0.9809\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 1.5808e-05 - accuracy: 1.0000 - val_loss: 0.1384 - val_accuracy: 0.9810\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 1.3099e-05 - accuracy: 1.0000 - val_loss: 0.1399 - val_accuracy: 0.9811\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 1.1091e-05 - accuracy: 1.0000 - val_loss: 0.1412 - val_accuracy: 0.9813\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 9.1655e-06 - accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9812\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 7.7902e-06 - accuracy: 1.0000 - val_loss: 0.1445 - val_accuracy: 0.9812\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 6.3016e-06 - accuracy: 1.0000 - val_loss: 0.1465 - val_accuracy: 0.9811\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 5.1460e-06 - accuracy: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.9807\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 4.3807e-06 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9809\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 3.5897e-06 - accuracy: 1.0000 - val_loss: 0.1522 - val_accuracy: 0.9812\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 2.9256e-06 - accuracy: 1.0000 - val_loss: 0.1541 - val_accuracy: 0.9813\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 2.2643e-06 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9805\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 1.8744e-06 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9813\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 1.5206e-06 - accuracy: 1.0000 - val_loss: 0.1618 - val_accuracy: 0.9805\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0294 - accuracy: 0.9927 - val_loss: 0.1712 - val_accuracy: 0.9750\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.1620 - val_accuracy: 0.9776\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.1689 - val_accuracy: 0.9772\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.1955 - val_accuracy: 0.9766\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.1983 - val_accuracy: 0.9744\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0032 - accuracy: 0.9988 - val_loss: 0.1664 - val_accuracy: 0.9772\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.1703 - val_accuracy: 0.9777\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 7.6718e-04 - accuracy: 0.9998 - val_loss: 0.1575 - val_accuracy: 0.9784\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.0682e-04 - accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9791\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.4035e-05 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9794\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.6325e-05 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9790\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 1.3305e-05 - accuracy: 1.0000 - val_loss: 0.1599 - val_accuracy: 0.9790\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.1208e-05 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9790\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 9.3073e-06 - accuracy: 1.0000 - val_loss: 0.1615 - val_accuracy: 0.9789\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.7968e-06 - accuracy: 1.0000 - val_loss: 0.1621 - val_accuracy: 0.9791\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 6.5738e-06 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9790\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 5.4538e-06 - accuracy: 1.0000 - val_loss: 0.1654 - val_accuracy: 0.9792\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 4.4648e-06 - accuracy: 1.0000 - val_loss: 0.1667 - val_accuracy: 0.9795\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 3.7328e-06 - accuracy: 1.0000 - val_loss: 0.1686 - val_accuracy: 0.9794\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 3.0783e-06 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9796\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 2.5775e-06 - accuracy: 1.0000 - val_loss: 0.1727 - val_accuracy: 0.9798\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 2.1223e-06 - accuracy: 1.0000 - val_loss: 0.1736 - val_accuracy: 0.9800\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.7874e-06 - accuracy: 1.0000 - val_loss: 0.1746 - val_accuracy: 0.9798\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.4400e-06 - accuracy: 1.0000 - val_loss: 0.1777 - val_accuracy: 0.9797\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.1940e-06 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9800\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 9.9058e-07 - accuracy: 1.0000 - val_loss: 0.1813 - val_accuracy: 0.9801\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 7.9346e-07 - accuracy: 1.0000 - val_loss: 0.1828 - val_accuracy: 0.9800\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 6.5285e-07 - accuracy: 1.0000 - val_loss: 0.1843 - val_accuracy: 0.9797\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 5.4535e-07 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9800\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 4.3098e-07 - accuracy: 1.0000 - val_loss: 0.1873 - val_accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "## convert class vectors to binary class matrices\n",
    "y_train_c = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_c = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test_c = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(180, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense( 40, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_c,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1736070215702057\n",
      "Test accuracy: 0.9821000099182129\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test_c, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the training going through the epochs and in the end the trained network is evaluated on the test set. \n",
    "It shoud reach at least a classification accurary of 97% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now try to tune the hyper-parameters of the MLP to achieve more than 98% accuracy on the test set.\\\n",
    "List the parameters you changed to achieve this score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
